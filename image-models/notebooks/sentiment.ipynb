{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Multimodal sentiment detection dataset.\n",
    "\n",
    "the files are here: https://www.dropbox.com/s/hpkdyuui5adqsek/multimodal_sentiment_dataset.zip?dl=0\n",
    "the dataset comes from this paper: https://arxiv.org/abs/1708.02099\n",
    "There are four classes ['creepy', 'gore', 'happy', 'rage'} (it'd be good if you could maintain this order as well)\n",
    "The dropbox also contains the train, val, and test split. The images are in the subdirectory called images , and are indexed as per the id in the .pkl files. For instance if an example has id <n>, the corresponding image would be ./images/<n>.jpg/png/jpeg (the file format may vary for some of the images)\n",
    "We are reporting the macro F1, precision, recall (and accuracy) – just like we did for the crisis dataset\n",
    "\n",
    "Multimodal fakenews detection dataset.\n",
    " \n",
    "the files are here: https://www.dropbox.com/s/esnr2ltp0z2ibq8/multmodal_fakenews_dataset.zip?dl=0\n",
    "the dataset comes from this paper: https://ojs.aaai.org/index.php/AAAI/article/view/7230\n",
    "There are two classes: 0 (not fake) and 1 (fake). Since this is binary classification task, we don't need to report macro-averaged scores. We can report the F1, precision and recall for the target class (i.e., 1/fake). In sklearn it can be specified by specifying average = 'binary' .\n",
    "The dropbox contains the images as well as the .pkl files for train and test set. There's no validation set, but I used 10% randomly sampled examples from the train set as my validation set. Also, please use the image_filename str (5vthXfxXiBvcdhZRirjpnhDJJi14UqtR in the following example)  as the id for any given example.\n",
    "{'title': \"Teen Mom Star Jenelle Evans' Wedding Dress Is Available Here for $2999\", 'image': '5vthXfxXiBvcdhZRirjpnhDJJi14UqtR.jpg', 'label': 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import stanza\n",
    "import cv2\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import time\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, Sampler, \\\n",
    "    SequentialSampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from json import load, dump\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./data/final_dataset_train.pkl', 'rb'))\n",
    "vali_data = pickle.load(open('./data/final_dataset_val.pkl', 'rb'))\n",
    "test_data = pickle.load(open('./data/final_dataset_test.pkl', 'rb'))\n",
    "\n",
    "encoder = {'creepy': 0, 'gore': 1, 'happy': 2, 'rage': 3}\n",
    "\n",
    "train_data_pairs = [[d['image'], encoder[d['label']]] for d in train_data]\n",
    "val_data_pairs = [[d['image'], encoder[d['label']]] for d in train_data]\n",
    "test_data_pairs = [[d['image'], encoder[d['label']]] for d in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_pairs, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the train/test/dev split tsv file\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.data_pairs = data_pairs\n",
    "        self.root_dir = root_dir\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.resize = transforms.Resize(256)\n",
    "        self.center = transforms.CenterCrop(224)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.data_pairs[idx][0]) + '.jpg'\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except:\n",
    "            idx = 0\n",
    "            img_name = os.path.join(self.root_dir, self.data_pairs[0][0]) + '.jpg'\n",
    "            image = Image.open(img_name).convert('RGB') \n",
    "        \n",
    "        image = self.resize(image)\n",
    "        image = self.center(image)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data_pairs[idx][1]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_data_pairs, './data/images')\n",
    "val_dataset = ImageDataset(val_data_pairs, './data/images')\n",
    "test_dataset = ImageDataset(test_data_pairs, './data/images')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "val_dataset_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) tensor(2.6400) tensor([0, 0, 3, 1, 0, 3, 3, 2, 1, 3, 2, 0, 0, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 1,\n",
      "        3, 3, 2, 1, 1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_dataset_loader:\n",
    "    print(inputs.shape, torch.max(inputs), labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, patience=20):\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Early stopping\n",
    "    es_counter = 0\n",
    "    es = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if es:\n",
    "            break\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    es_counter = 0\n",
    "                else:\n",
    "                    es_counter += 1\n",
    "                    if es_counter > patience:\n",
    "                        es = True\n",
    "\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = {'train': train_dataset_loader,\n",
    "#                'val': val_dataset_loader}\n",
    "\n",
    "# model, val_acc_history = train_model(model, dataloaders, criterion,\n",
    "#                                      optimizer_ft, num_epochs=50,\n",
    "#                                      patience=10)\n",
    "\n",
    "# torch.save(model.state_dict(), './output/best_weights_sentiment.pk')\n",
    "# # dump(val_acc_history, open('./output/val_acc_history.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_sentiment.pk'))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "true_labels = []\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for inputs, labels, tids in tqdm(test_dataset_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy().tolist())\n",
    "    true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "confusion = metrics.confusion_matrix(true_labels, predictions)\n",
    "\n",
    "ticks = ['creepy', 'gore', 'happy', 'rage']\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "ax = sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"YlGnBu\", square=True,\n",
    "                 xticklabels=ticks, yticklabels=ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94898753894081 0.947383701404479 0.9477082648548574 0.9470914857008083\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.accuracy_score(true_labels, predictions),\n",
    "    metrics.f1_score(true_labels, predictions, average='macro'),\n",
    "    metrics.precision_score(true_labels, predictions, average='macro'),\n",
    "    metrics.recall_score(true_labels, predictions, average='macro'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./data/final_dataset_train.pkl', 'rb'))\n",
    "vali_data = pickle.load(open('./data/final_dataset_val.pkl', 'rb'))\n",
    "test_data = pickle.load(open('./data/final_dataset_test.pkl', 'rb'))\n",
    "\n",
    "encoder = {'creepy': 0, 'gore': 1, 'happy': 2, 'rage': 3}\n",
    "\n",
    "train_data_pairs = [[d['image'], encoder[d['label']]] for d in train_data]\n",
    "val_data_pairs = [[d['image'], encoder[d['label']]] for d in vali_data]\n",
    "test_data_pairs = [[d['image'], encoder[d['label']]] for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': '485',\n",
       " 'text': \"Crazy things you can do with a lil' fake blood and one leg\",\n",
       " 'label': 'creepy'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_pairs, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the train/test/dev split tsv file\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.data_pairs = data_pairs\n",
    "        self.root_dir = root_dir\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.resize = transforms.Resize(256)\n",
    "        self.center = transforms.CenterCrop(224)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.data_pairs[idx][0]) + '.jpg'\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except:\n",
    "            idx = 0\n",
    "            img_name = os.path.join(self.root_dir, self.data_pairs[0][0]) + '.jpg'\n",
    "            image = Image.open(img_name).convert('RGB') \n",
    "        \n",
    "        image = self.resize(image)\n",
    "        image = self.center(image)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data_pairs[idx][1]\n",
    "        \n",
    "        curID = self.data_pairs[idx][0]\n",
    "        \n",
    "        return image, label, curID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_data_pairs, './data/images')\n",
    "val_dataset = ImageDataset(val_data_pairs, './data/images')\n",
    "test_dataset = ImageDataset(test_data_pairs, './data/images')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "val_dataset_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "num_classes = 4\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_sentiment.pk'))\n",
    "\n",
    "output_256 = []\n",
    "output_256_tid = []\n",
    "\n",
    "def get_output(self, input, output):\n",
    "    cur_output = output.data.detach().cpu().numpy()\n",
    "    for i in range(cur_output.shape[0]):\n",
    "        output_256.append(cur_output[i, :])\n",
    "\n",
    "model.classifier._modules['6'][0].register_forward_hook(get_output)\n",
    "\n",
    "for inputs, labels, tids in tqdm(train_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "    output_256_tid.extend(list(tids))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./data/sentiment/image_embedding_256_train.npz',\n",
    "                    image_embedding=output_256,\n",
    "                    image_ID=output_256_tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:19<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "num_classes = 4\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_sentiment.pk'))\n",
    "\n",
    "output_256 = []\n",
    "output_256_tid = []\n",
    "\n",
    "def get_output(self, input, output):\n",
    "    cur_output = output.data.detach().cpu().numpy()\n",
    "    for i in range(cur_output.shape[0]):\n",
    "        output_256.append(cur_output[i, :])\n",
    "\n",
    "model.classifier._modules['6'][0].register_forward_hook(get_output)\n",
    "\n",
    "for inputs, labels, tids in tqdm(val_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "    output_256_tid.extend(list(tids))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "np.savez_compressed('./data/sentiment/image_embedding_256_val.npz',\n",
    "                    image_embedding=output_256,\n",
    "                    image_ID=output_256_tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "num_classes = 4\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_sentiment.pk'))\n",
    "\n",
    "output_256 = []\n",
    "output_256_tid = []\n",
    "\n",
    "def get_output(self, input, output):\n",
    "    cur_output = output.data.detach().cpu().numpy()\n",
    "    for i in range(cur_output.shape[0]):\n",
    "        output_256.append(cur_output[i, :])\n",
    "\n",
    "model.classifier._modules['6'][0].register_forward_hook(get_output)\n",
    "\n",
    "for inputs, labels, tids in tqdm(test_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "    output_256_tid.extend(list(tids))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "np.savez_compressed('./data/sentiment/image_embedding_256_test.npz',\n",
    "                    image_embedding=output_256,\n",
    "                    image_ID=output_256_tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, 4))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_sentiment.pk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "test_probs = []\n",
    "test_probs_tids = []\n",
    "\n",
    "for inputs, labels, tids in tqdm(test_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    softmax_outputs = nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    test_probs.append(softmax_outputs)\n",
    "    \n",
    "    test_probs_tids.extend(list(tids))\n",
    "\n",
    "test_probs = np.vstack(test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./data/sentiment/image_predict_probs_test.npz',\n",
    "                    predict_probs=test_probs,\n",
    "                    image_id=test_probs_tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:19<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "val_probs = []\n",
    "val_probs_tids = []\n",
    "\n",
    "for inputs, labels, tids in tqdm(val_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    softmax_outputs = nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    val_probs.append(softmax_outputs)\n",
    "    \n",
    "    val_probs_tids.extend(list(tids))\n",
    "\n",
    "val_probs_tids = np.vstack(val_probs_tids)\n",
    "\n",
    "np.savez_compressed('./data/sentiment/image_predict_probs_val.npz',\n",
    "                    predict_probs=val_probs,\n",
    "                    image_id=val_probs_tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [02:34<00:00,  1.90s/it]\n",
      "/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "val_probs = []\n",
    "val_probs_tids = []\n",
    "\n",
    "for inputs, labels, tids in tqdm(train_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    softmax_outputs = nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    val_probs.append(softmax_outputs)\n",
    "    \n",
    "    val_probs_tids.extend(list(tids))\n",
    "\n",
    "val_probs_tids = np.vstack(val_probs_tids)\n",
    "\n",
    "np.savez_compressed('./data/sentiment/image_predict_probs_train.npz',\n",
    "                    predict_probs=val_probs,\n",
    "                    image_id=val_probs_tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
