{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Multimodal sentiment detection dataset.\n",
    "\n",
    "the files are here: https://www.dropbox.com/s/hpkdyuui5adqsek/multimodal_sentiment_dataset.zip?dl=0\n",
    "the dataset comes from this paper: https://arxiv.org/abs/1708.02099\n",
    "There are four classes ['creepy', 'gore', 'happy', 'rage'} (it'd be good if you could maintain this order as well)\n",
    "The dropbox also contains the train, val, and test split. The images are in the subdirectory called images , and are indexed as per the id in the .pkl files. For instance if an example has id <n>, the corresponding image would be ./images/<n>.jpg/png/jpeg (the file format may vary for some of the images)\n",
    "We are reporting the macro F1, precision, recall (and accuracy) – just like we did for the crisis dataset\n",
    "\n",
    "Multimodal fakenews detection dataset.\n",
    " \n",
    "the files are here: https://www.dropbox.com/s/esnr2ltp0z2ibq8/multmodal_fakenews_dataset.zip?dl=0\n",
    "the dataset comes from this paper: https://ojs.aaai.org/index.php/AAAI/article/view/7230\n",
    "There are two classes: 0 (not fake) and 1 (fake). Since this is binary classification task, we don't need to report macro-averaged scores. We can report the F1, precision and recall for the target class (i.e., 1/fake). In sklearn it can be specified by specifying average = 'binary' .\n",
    "The dropbox contains the images as well as the .pkl files for train and test set. There's no validation set, but I used 10% randomly sampled examples from the train set as my validation set. Also, please use the image_filename str (5vthXfxXiBvcdhZRirjpnhDJJi14UqtR in the following example)  as the id for any given example.\n",
    "{'title': \"Teen Mom Star Jenelle Evans' Wedding Dress Is Available Here for $2999\", 'image': '5vthXfxXiBvcdhZRirjpnhDJJi14UqtR.jpg', 'label': 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import stanza\n",
    "import cv2\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "\n",
    "from os.path import basename, join\n",
    "from shutil import copyfile\n",
    "\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, Sampler, \\\n",
    "    SequentialSampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from json import load, dump\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./data/fake-news-data/train_examples.pkl', 'rb'))\n",
    "test_data = pickle.load(open('./data/fake-news-data/test_examples.pkl', 'rb'))\n",
    "\n",
    "val_indexes = np.random.choice(range(len(train_data)),\n",
    "                               int(len(train_data) * 0.1),\n",
    "                               replace=False)\n",
    "train_indexes = list(set(range(len(train_data))).difference(set(val_indexes.tolist())))\n",
    "\n",
    "val_data = np.array(train_data)[val_indexes]\n",
    "train_data = np.array(train_data)[train_indexes]\n",
    "\n",
    "train_data_pairs = [[d['image'], - d['label'] + 1] for d in train_data]\n",
    "val_data_pairs = [[d['image'], - d['label'] + 1] for d in val_data]\n",
    "test_data_pairs = [[d['image'], - d['label'] + 1] for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in tqdm(glob('./data/fake-news-data/AAAI_dataset/Images/*/*')):\n",
    "#     name = basename(f)\n",
    "#     copyfile(f, './data/fake-news-data/images/{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7492, 1: 2010})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([i[1] for i in train_data_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_pairs, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the train/test/dev split tsv file\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.data_pairs = data_pairs\n",
    "        self.root_dir = root_dir\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.resize = transforms.Resize(256)\n",
    "        self.center = transforms.CenterCrop(224)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.data_pairs[idx][0])\n",
    "        \n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        image = self.resize(image)\n",
    "        image = self.center(image)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data_pairs[idx][1]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_data_pairs, './data/fake-news-data/images')\n",
    "val_dataset = ImageDataset(val_data_pairs, './data/fake-news-data/images')\n",
    "test_dataset = ImageDataset(test_data_pairs, './data/fake-news-data/images')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "val_dataset_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) tensor(2.6400) tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_dataset_loader:\n",
    "    print(inputs.shape, torch.max(inputs), labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, patience=20):\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Early stopping\n",
    "    es_counter = 0\n",
    "    es = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if es:\n",
    "            break\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    es_counter = 0\n",
    "                else:\n",
    "                    es_counter += 1\n",
    "                    if es_counter > patience:\n",
    "                        es = True\n",
    "\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = {'train': train_dataset_loader,\n",
    "#                'val': val_dataset_loader}\n",
    "\n",
    "# model, val_acc_history = train_model(model, dataloaders, criterion,\n",
    "#                                      optimizer_ft, num_epochs=50,\n",
    "#                                      patience=10)\n",
    "\n",
    "# torch.save(model.state_dict(), './output/best_weights_fake.pk')\n",
    "# # dump(val_acc_history, open('./output/val_acc_history.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_fake.pk'))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 21/84 [00:39<01:45,  1.67s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:785: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 43%|████▎     | 36/84 [01:07<01:25,  1.79s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 84/84 [02:30<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "true_labels = []\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for inputs, labels in tqdm(test_dataset_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy().tolist())\n",
    "    true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "confusion = metrics.confusion_matrix(true_labels, predictions)\n",
    "\n",
    "ticks = ['not fake', 'fake']\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "ax = sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"YlGnBu\", square=True,\n",
    "                 xticklabels=ticks, yticklabels=ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8116858950502419 0.15384615384615385 0.5411764705882353 0.08966861598440545\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.accuracy_score(true_labels, predictions),\n",
    "    metrics.f1_score(true_labels, predictions, average='binary'),\n",
    "    metrics.precision_score(true_labels, predictions, average='binary'),\n",
    "    metrics.recall_score(true_labels, predictions, average='binary'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./data/fake-news-data/train_examples.pkl', 'rb'))\n",
    "test_data = pickle.load(open('./data/fake-news-data/test_examples.pkl', 'rb'))\n",
    "\n",
    "# val_indexes = np.random.choice(range(len(train_data)),\n",
    "#                                int(len(train_data) * 0.1),\n",
    "#                                replace=False)\n",
    "# train_indexes = list(set(range(len(train_data))).difference(set(val_indexes.tolist())))\n",
    "\n",
    "# val_data = np.array(train_data)[val_indexes]\n",
    "# train_data = np.array(train_data)[train_indexes]\n",
    "\n",
    "train_data_pairs = [[d['image'], 1 - d['label']] for d in train_data]\n",
    "# val_data_pairs = [[d['image'], d['label']] for d in val_data]\n",
    "test_data_pairs = [[d['image'], 1 - d['label']] for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Kylie Jenner refusing to discuss Tyga on Life of Kylie',\n",
       " 'image': '4muaHBEpDomEuc353G1ROLrfA5RbkmaI.jpg',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_pairs, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the train/test/dev split tsv file\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.data_pairs = data_pairs\n",
    "        self.root_dir = root_dir\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.resize = transforms.Resize(256)\n",
    "        self.center = transforms.CenterCrop(224)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.data_pairs[idx][0])\n",
    "        \n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        image = self.resize(image)\n",
    "        image = self.center(image)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data_pairs[idx][1]\n",
    "        \n",
    "        # Get the id\n",
    "        image_id = self.data_pairs[idx][0].replace('.jpg', '')\n",
    "        \n",
    "        return image, label, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_data_pairs, './data/fake-news-data/images')\n",
    "# val_dataset = ImageDataset(val_data_pairs, './data/images')\n",
    "test_dataset = ImageDataset(test_data_pairs, './data/fake-news-data/images')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "# val_dataset_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "#                                                  batch_size=batch_size,\n",
    "#                                                  shuffle=True,\n",
    "#                                                  num_workers=4)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 31/330 [00:59<08:31,  1.71s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 12%|█▏        | 38/330 [01:12<08:33,  1.76s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 17%|█▋        | 56/330 [01:43<08:06,  1.78s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 20%|█▉        | 65/330 [01:58<07:28,  1.69s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 330/330 [10:54<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "num_classes = 2\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_fake.pk'))\n",
    "\n",
    "output_256 = []\n",
    "output_256_tid = []\n",
    "\n",
    "def get_output(self, input, output):\n",
    "    cur_output = output.data.detach().cpu().numpy()\n",
    "    for i in range(cur_output.shape[0]):\n",
    "        output_256.append(cur_output[i, :])\n",
    "\n",
    "model.classifier._modules['6'][0].register_forward_hook(get_output)\n",
    "\n",
    "for inputs, labels, tids in tqdm(train_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "    output_256_tid.extend(list(tids))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./data/fake-news/image_embedding_256_train.npz',\n",
    "                    image_embedding=output_256,\n",
    "                    image_ID=output_256_tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 51%|█████     | 43/84 [01:16<01:10,  1.73s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 60%|█████▉    | 50/84 [01:28<00:58,  1.71s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 86%|████████▌ | 72/84 [02:06<00:21,  1.76s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:785: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 84/84 [02:27<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "num_classes = 2\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_fake.pk'))\n",
    "\n",
    "output_256 = []\n",
    "output_256_tid = []\n",
    "\n",
    "def get_output(self, input, output):\n",
    "    cur_output = output.data.detach().cpu().numpy()\n",
    "    for i in range(cur_output.shape[0]):\n",
    "        output_256.append(cur_output[i, :])\n",
    "\n",
    "model.classifier._modules['6'][0].register_forward_hook(get_output)\n",
    "\n",
    "for inputs, labels, tids in tqdm(test_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "    output_256_tid.extend(list(tids))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "np.savez_compressed('./data/fake-news/image_embedding_256_test.npz',\n",
    "                    image_embedding=output_256,\n",
    "                    image_ID=output_256_tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(256, 2))\n",
    "\n",
    "model.load_state_dict(torch.load('./output/best_weights_fake.pk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 14%|█▍        | 12/84 [00:21<02:03,  1.72s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 42%|████▏     | 35/84 [01:02<01:27,  1.79s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:785: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 84/84 [02:27<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "test_probs = []\n",
    "test_probs_tids = []\n",
    "\n",
    "for inputs, labels, tids in tqdm(test_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    softmax_outputs = nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    test_probs.append(softmax_outputs)\n",
    "    \n",
    "    test_probs_tids.extend(list(tids))\n",
    "\n",
    "test_probs = np.vstack(test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./data/fake-news/image_predict_probs_test.npz',\n",
    "                    predict_probs=test_probs,\n",
    "                    image_id=test_probs_tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 106/330 [03:14<07:06,  1.90s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 45%|████▍     | 147/330 [04:27<05:34,  1.83s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 55%|█████▍    | 180/330 [05:29<04:48,  1.92s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      " 68%|██████▊   | 225/330 [06:51<03:05,  1.77s/it]/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 330/330 [10:02<00:00,  1.83s/it]\n",
      "/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "val_probs = []\n",
    "val_probs_tids = []\n",
    "\n",
    "for inputs, labels, tids in tqdm(train_dataset_loader):\n",
    "    inputs = inputs\n",
    "    labels = labels\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    softmax_outputs = nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    val_probs.append(softmax_outputs)\n",
    "    \n",
    "    val_probs_tids.extend(list(tids))\n",
    "\n",
    "val_probs_tids = np.vstack(val_probs_tids)\n",
    "\n",
    "np.savez_compressed('./data/sentiment/image_predict_probs_train.npz',\n",
    "                    predict_probs=val_probs,\n",
    "                    image_id=val_probs_tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [02:34<00:00,  1.90s/it]\n",
      "/nethome/zwang3049/jay/miniconda3/envs/nlp/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
