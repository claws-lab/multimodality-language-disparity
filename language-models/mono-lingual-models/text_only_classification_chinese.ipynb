{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text-only-classification-chinese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we train the Chinese language text-only classification models for Humanitarian Classification Task"
      ],
      "metadata": {
        "id": "I4V3avBozOhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install libraries needed for the task."
      ],
      "metadata": {
        "id": "rkpKbu446pGW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJQh2cy6f-_X",
        "outputId": "906a8bce-57b4-4802-bb22-dc0ebdc019f1"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install transformers\n",
        "!pip install ftfy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 13.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the [CrisisMMD](https://arxiv.org/abs/1805.00713) dataset comprising of 7216 Twitter posts (images + text) that are categorized into 5 humanitarian categories. We store the dataset (eg filename for DEV set : `task_humanitarian_text_img_dev.tsv`) on a Google Drive at the (relative) path `/content/drive/My Drive/crisis_bert/data/` and mount the drive at this path. We are assuming that this notebook is stored at the path /content/drive/My Drive/crisis_bert/code"
      ],
      "metadata": {
        "id": "BhHBNuw0zM2o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfOvbllZorfJ",
        "outputId": "4c957da2-f964-4c61-c9d2-51bb30a2e93f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/crisis_bert/code')\n",
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/crisis_bert/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the train, test and dev files, we preprocess the data and and pickle and store the preprocessed files at `/content/drive/My Drive/crisis_bert/data`"
      ],
      "metadata": {
        "id": "5MkmJvJ9_DMO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95rQDJ7kik3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08992c5-05a8-42a5-e8a7-c22185301048"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import preprocessor as tp\n",
        "import ftfy\n",
        "\n",
        "def FormatText(filename):\n",
        "    data_list = []\n",
        "    data = pd.read_csv(filename, sep = '\\t')\n",
        "\n",
        "    # Clean the \"tweet_text\" column\n",
        "    tp.set_options(tp.OPT.URL, tp.OPT.EMOJI, tp.OPT.SMILEY, tp.OPT.RESERVED)\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].apply(lambda x: tp.clean(x))\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].apply(lambda x : ftfy.fix_text(x))\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(r'\\\\n',' ', regex=True) \n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(r\"\\'t\", \" not\")\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].str.strip()\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(\"#\",\"\")\n",
        "    data[\"tweet_text\"] = data[\"tweet_text\"].str.replace(\"@\",\"\")\n",
        "    tweet_id = data['tweet_id'].to_list()\n",
        "    image_id = data['image_id'].to_list()\n",
        "    tweet_text = data['tweet_text'].to_list()\n",
        "    tweet_text = [str(x) for x in tweet_text]\n",
        "\n",
        "    label = data['label'].to_list()\n",
        "    alignment = data['label_text_image'].to_list()\n",
        "    for a_var in range(len(tweet_id)):\n",
        "        data_point = {}\n",
        "        if alignment[a_var] == 'Positive':\n",
        "            data_point['tweet_id'] = tweet_id[a_var]\n",
        "            data_point['image_id'] = image_id[a_var]\n",
        "            data_point['tweet_text'] = tweet_text[a_var]\n",
        "            data_point['label'] = label[a_var] #these labels are not final, need to be updated in a downstream script\n",
        "            data_list.append(data_point)\n",
        "    return data_list\n",
        "\n",
        "folderpath = '../data/'\n",
        "filenames = ['task_humanitarian_text_img_dev.tsv', 'task_humanitarian_text_img_test.tsv', 'task_humanitarian_text_img_train.tsv']\n",
        "\n",
        "for a_file in filenames:\n",
        "    data = FormatText(folderpath + a_file)\n",
        "    source_text = [x['tweet_text'] for x in data]\n",
        "    for a_var in range(len(data)):\n",
        "        data[a_var]['tweet_text'] = [source_text[a_var]]\n",
        "    with open(folderpath + a_file.split('.')[0] + '.pkl', 'wb') as f:\n",
        "        pkl.dump(data, f)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use these pickled files to convert the preprocessed data to a suitable input for building and training the model."
      ],
      "metadata": {
        "id": "HMI0tpYszjts"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb3wL8xI1dXe"
      },
      "source": [
        "import os, sys, re\n",
        "import torch\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "NUM_LABELS = 5\n",
        "\n",
        "class DatasetFormatting(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def AlignFormatLabels(list, labels):\n",
        "    aligned_list = []\n",
        "    for item in list:\n",
        "        if item == 'vehicle_damage':\n",
        "            aligned_list.append('infrastructure_and_utility_damage')\n",
        "        elif item == 'missing_or_found_people' or item == 'injured_or_dead_people':\n",
        "            aligned_list.append('affected_individuals')\n",
        "        else:\n",
        "            aligned_list.append(item)\n",
        "    final_labels = []\n",
        "    for item in aligned_list:\n",
        "        final_labels.append(labels.index(item))\n",
        "    return final_labels\n",
        "\n",
        "def GetData(folder):\n",
        "    filenames_prefix = 'task_humanitarian_text_img_'\n",
        "    with open(folder + filenames_prefix + 'train.pkl', 'rb') as f:\n",
        "        train_data = pkl.load(f)\n",
        "    with open(folder + filenames_prefix + 'dev.pkl', 'rb') as f:\n",
        "        val_data = pkl.load(f)\n",
        "    with open(folder + filenames_prefix + 'test.pkl', 'rb') as f:\n",
        "        test_data = pkl.load(f)\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    train_ids = []\n",
        "    for a_point in train_data:\n",
        "        # print(a_point)\n",
        "        train_ids.append(a_point['tweet_id'])\n",
        "        train_texts.append(a_point['tweet_text'])\n",
        "        train_labels.append(a_point['label'])\n",
        "    val_texts = []\n",
        "    val_labels = []\n",
        "    val_ids = []\n",
        "    for a_point in val_data:\n",
        "        val_ids.append(a_point['tweet_id'])\n",
        "        val_texts.append(a_point['tweet_text'])\n",
        "        val_labels.append(a_point['label'])\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    test_ids = []\n",
        "    for a_point in test_data:\n",
        "        test_ids.append(a_point['tweet_id'])\n",
        "        test_texts.append(a_point['tweet_text'])\n",
        "        test_labels.append(a_point['label'])\n",
        "    my_labels = ['other_relevant_information', 'affected_individuals', 'rescue_volunteering_or_donation_effort', 'infrastructure_and_utility_damage', 'not_humanitarian']\n",
        "    \n",
        "    train_texts = [x[0] for x in train_texts]\n",
        "    val_texts = [x[0] for x in val_texts]\n",
        "    test_texts = [x[0] for x in test_texts]\n",
        "    train_labels = AlignFormatLabels(train_labels, my_labels)\n",
        "    val_labels = AlignFormatLabels(val_labels, my_labels)\n",
        "    test_labels = AlignFormatLabels(test_labels, my_labels)\n",
        "    return train_texts, train_labels, train_ids, val_texts, val_labels, val_ids, test_texts, test_labels, test_ids\n",
        "\n",
        "pickle_folder_path = '../data/'\n",
        "train_texts, train_labels, train_ids, val_texts, val_labels, val_ids, test_texts, test_labels, test_ids = GetData(pickle_folder_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "train_df = pd.DataFrame({'text': train_texts,'label': train_labels})\n",
        "eval_df = pd.DataFrame({'text': val_texts,'label': val_labels})\n",
        "test_df = pd.DataFrame({'text': test_texts,'label': test_labels})\n",
        "train_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R7yGJ_mf2iPr",
        "outputId": "5e4d9e55-fcd3-4889-d98a-12a08361c558"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  KAKEnews: California wildfires destroy more th...      3\n",
              "1  KAKEnews: California wildfires destroy more th...      3\n",
              "2  KAKEnews: California wildfires destroy more th...      3\n",
              "3  TheAtlantic: Photos of California's destructiv...      3\n",
              "4  Why California's wildfires are worse in the fall.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e1581e5-ef34-49e8-8ea7-74ece941fd88\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KAKEnews: California wildfires destroy more th...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KAKEnews: California wildfires destroy more th...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KAKEnews: California wildfires destroy more th...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TheAtlantic: Photos of California's destructiv...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why California's wildfires are worse in the fall.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1581e5-ef34-49e8-8ea7-74ece941fd88')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e1581e5-ef34-49e8-8ea7-74ece941fd88 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e1581e5-ef34-49e8-8ea7-74ece941fd88');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import sklearn\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = DatasetFormatting(train_encodings, train_labels)\n",
        "val_dataset = DatasetFormatting(val_encodings, val_labels)\n",
        "test_dataset = DatasetFormatting(test_encodings, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KBOL31w0Z5I",
        "outputId": "2dca52df-ce44-442e-e93e-3b98a6e59721"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz5bq_8mTRpL"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    print(pred.predictions)\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For demo purposes, we train only for 2 epochs."
      ],
      "metadata": {
        "id": "xysg35QFwhuo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eE9uLDts2X5Z",
        "outputId": "d7f19c64-b947-400e-c411-41d31704f3aa"
      },
      "source": [
        "#comment this line to enable logging of weights and biases \n",
        "#(you might need a wandb account and an API key for this!)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_',          # output directory\n",
        "    num_train_epochs=2 ,             # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batchx size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    # learning_rate = 5e-5\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    save_steps = 500,\n",
        "    evaluation_strategy='epoch'\n",
        ")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm', num_labels=NUM_LABELS)\n",
        "\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    tokenizer = tokenizer,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer.train()\n",
        "# trainer.train()\n",
        "trainer.save_model('trained_models')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6126\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1532\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1532' max='1532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1532/1532 03:44, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.900500</td>\n",
              "      <td>0.808998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.560200</td>\n",
              "      <td>0.641484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results_/checkpoint-500\n",
            "Configuration saved in ./results_/checkpoint-500/config.json\n",
            "Model weights saved in ./results_/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./results_/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./results_/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 998\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results_/checkpoint-1000\n",
            "Configuration saved in ./results_/checkpoint-1000/config.json\n",
            "Model weights saved in ./results_/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./results_/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./results_/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to ./results_/checkpoint-1500\n",
            "Configuration saved in ./results_/checkpoint-1500/config.json\n",
            "Model weights saved in ./results_/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./results_/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./results_/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 998\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to trained_models\n",
            "Configuration saved in trained_models/config.json\n",
            "Model weights saved in trained_models/pytorch_model.bin\n",
            "tokenizer config file saved in trained_models/tokenizer_config.json\n",
            "Special tokens file saved in trained_models/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g-yddHcyOf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c5026f-307a-4fab-93bc-b7f94a52e6d0"
      },
      "source": [
        "trainer.save_model('trained_models')\n",
        "torch.save(model.state_dict(), 'trained_models_new/trained_model_chinese.pt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to trained_models\n",
            "Configuration saved in trained_models/config.json\n",
            "Model weights saved in trained_models/pytorch_model.bin\n",
            "tokenizer config file saved in trained_models/tokenizer_config.json\n",
            "Special tokens file saved in trained_models/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs, _, metrics = trainer.predict(test_dataset)\n",
        "pred_labels = probs[0].argmax(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "ZpmPGK3ay_s9",
        "outputId": "ff551ea8-cccd-400d-f093-fe2eee9e708e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 955\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names=['other_relevant_information', 'affected_individuals', 'rescue_volunteering_or_donation_effort', 'infrastructure_and_utility_damage', 'not_humanitarian']\n",
        "print(classification_report(test_labels, pred_labels ,target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eIWiB_UxbAp",
        "outputId": "a85b6248-39cf-460e-97a1-f49c3f203cd2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "            other_relevant_information       0.77      0.73      0.75       235\n",
            "                  affected_individuals       0.00      0.00      0.00         9\n",
            "rescue_volunteering_or_donation_effort       0.69      0.74      0.71       126\n",
            "     infrastructure_and_utility_damage       0.68      0.64      0.66        81\n",
            "                      not_humanitarian       0.82      0.85      0.83       504\n",
            "\n",
            "                              accuracy                           0.78       955\n",
            "                             macro avg       0.59      0.59      0.59       955\n",
            "                          weighted avg       0.77      0.78      0.77       955\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}